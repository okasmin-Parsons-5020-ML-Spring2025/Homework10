{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# Homework10\n",
    "\n",
    "Exercises with text processing and NLP modeling\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Understand similarities and differences between the processes of working with text, images and tabular data\n",
    "- Practice with different methods of encoding and modeling text data\n",
    "- See different methods for extracting information or patterns from text datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r ./.devcontainer/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/text_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from data_utils import display_silhouette_plots, object_from_json_url\n",
    "from text_utils import get_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell it's gonna be a good homework from the number of imports.\n",
    "# ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have protein, need seasoning\n",
    "\n",
    "Let's create a model to help us season our foods. In the end, what we want is a model that receives a short list of ingredients and returns a list of seasonings or complementary ingredients for our original ingredients list.\n",
    "\n",
    "In order to do that we need a dataset of recipes. We'll load that into a text dataset where each recipe is a document and the ingredients are our document *tokens*.\n",
    "\n",
    "Let's take a look at the recipe dataset and become familiar with the data and how it's organized.\n",
    "\n",
    "We'll load our recipes and do a bit of exploratory data analysis to look for patterns first to see if this kind of modeling makes any sense.\n",
    "\n",
    "### Load Data\n",
    "\n",
    "Here's our dataset. Let's load it into an object for inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"https://raw.githubusercontent.com/PSAM-5020-2025S-A/5020-utils/refs/heads/main/datasets/text/recipes\"\n",
    "recipes_obj = object_from_json_url(f\"{DATAPATH}/recipes_min16.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Data\n",
    "\n",
    "How's the data organized?\n",
    "\n",
    "How many recipes do we have?\n",
    "\n",
    "Do all recipes have the same number of ingredients?\n",
    "\n",
    "Anything else stand out about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Look at Data here\n",
    "recipes_obj\n",
    "print(recipes_obj[0])\n",
    "print(recipes_obj[20])\n",
    "print(recipes_obj[100])\n",
    "\n",
    "# TODO: How many recipes\n",
    "print(len(recipes_obj))\n",
    "\n",
    "# TODO: How many ingredients do the shortest and longest recipes have?\n",
    "min = len(recipes_obj[0][\"ingredients\"])\n",
    "max = len(recipes_obj[0][\"ingredients\"])\n",
    "\n",
    "for recipe in recipes_obj:\n",
    "    num = len(recipe[\"ingredients\"])\n",
    "    if num < min:\n",
    "        min = num\n",
    "    if num > max:\n",
    "        max = num\n",
    "\n",
    "print(\"min\", min)  \n",
    "print(\"max\", max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Features\n",
    "\n",
    "Our dataset doesn't really have to be a `DataFrame` here. It can, but it doesn't have to be.\n",
    "\n",
    "Each recipe right now is described as a list of ingredients, but what we really want is a list of *sentences*, where each *sentence* is a Python `string` with all of the ingredients for a given recipe.\n",
    "\n",
    "Instead of:<br>```[\"salt\", \"baking soda\", \"water\", \"mushroom\"]```,\n",
    "\n",
    "we want:<br>```\"salt baking soda water mushroom\"```\n",
    "\n",
    "The `join()` function might help.\n",
    "\n",
    "Another thing to consider is wether we want to do anything special about multi-word ingredients, like *baking soda*.\n",
    "\n",
    "Do we want to let our vectorizer (spoiler) split that into two tokens, or do we want to guarantee that *baking* and *soda* always stay together? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(list):\n",
    "    words = []\n",
    "    for word in list:\n",
    "        parts = word.split(\" \")\n",
    "        if len(parts) == 1:\n",
    "            words.append(word)      \n",
    "        else:\n",
    "            words.append(\"-\".join(parts)) \n",
    "    return \" \".join(words)\n",
    "\n",
    "string1 = list_to_string([\"baking soda\", \"salt\", \"spicy spicy pepper\"])\n",
    "print(string1)\n",
    "\n",
    "string2 = list_to_string(['raisins', 'baking powder', 'egg', 'sugar', 'milk', 'flour'])\n",
    "print(string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "recipes = []\n",
    "# TODO: turn list of objects into list of strings\n",
    "for recipe in recipes_obj:\n",
    "    string_ingredients = list_to_string(recipe[\"ingredients\"])\n",
    "    recipes.append(string_ingredients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(recipes[0])\n",
    "display(recipes[20])\n",
    "display(recipes[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Data\n",
    "\n",
    "The fun part.\n",
    "\n",
    "Let's vectorize our list of ingredient strings into a sparse document matrix using `CountVectorizer` or `TfidfVectorizer`.\n",
    "\n",
    "The resulting matrix will have one row for each recipe, and the columns will encode the ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Vectorize ingredients from our recipe list\n",
    "tfid = TfidfVectorizer(stop_words=\"english\", min_df=5, max_df=0.75, max_features=10_000)\n",
    "ingredients_vectorized = tfid.fit_transform(recipes)\n",
    "\n",
    "# TODO: How many words are in our vocabulary?\n",
    "vocab = tfid.get_feature_names_out()\n",
    "print(len(vocab))\n",
    "display(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Data\n",
    "\n",
    "Now that we have our recipes/documents vectorized we can study them a little bit, and look for patterns.\n",
    "\n",
    "What happens if we cluster our recipes ? What do the cluster centers represent ?\n",
    "\n",
    "When might this be useful ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: cluster recipes\n",
    "mClust = KMeans(n_clusters=8, random_state=800)\n",
    "ingredients_km = mClust.fit_predict(ingredients_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mClust.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Centers\n",
    "\n",
    "Use the `get_top_words()` function to decode the `cluster_centers` back into ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Look at cluster centers\n",
    "display(get_top_words(mClust.cluster_centers_, vocab, min)[0])\n",
    "display(get_top_words(mClust.cluster_centers_, vocab, 12)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "What do these cluster centers represent ?<br>\n",
    "Is there anything interesting about recipe cluster centers ?<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "The clusters represent the most common ingredients in each recipe.\n",
    "There are some ingredients like oil, salt and pepper that appear in nearly every one of them. \n",
    "\n",
    "I also don't think my attempt at combining words with \"-\" seems to have worked for the vectorized list..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Clusters\n",
    "\n",
    "Let's plot our clusters to see if we have to adjust any of the clustering parameters.\n",
    "\n",
    "Since we can't plot in $500$ dimensions, we should use `TruncatedSVD` to look at our clusters in $2D$ and $3D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: TruncatedSVD to reduce the dimensions of our feature space\n",
    "svd = TruncatedSVD(n_components=3, random_state=1010)\n",
    "ingredients_svd = svd.fit_transform(ingredients_vectorized)\n",
    "ingredients_svd[:4]\n",
    "\n",
    "# TODO: plot clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ammended from wk08\n",
    "def plot_ingredient_clusters(features_3d, clusters, title=\"Recipes\"):\n",
    "    x = features_3d[:, 0]  # SVD1\n",
    "    y = features_3d[:, 1]  # SVD2\n",
    "    z = features_3d[:, 2]  # SVD3\n",
    "\n",
    "    # 2D: SVD1 vs SVD2\n",
    "    plt.figure()\n",
    "    plt.scatter(x, y, c=clusters, marker='o', alpha=0.5)\n",
    "    plt.title(f\"{title} clustering\")\n",
    "    plt.xlabel(\"SVD1\")\n",
    "    plt.ylabel(\"SVD2\")\n",
    "    plt.ylim([-2.2, 3])\n",
    "    plt.show()\n",
    "\n",
    "    # 2D: SVD1 vs SVD3\n",
    "    plt.figure()\n",
    "    plt.scatter(x, z, c=clusters, marker='o', alpha=0.5)\n",
    "    plt.title(f\"{title} clustering\")\n",
    "    plt.xlabel(\"SVD1\")\n",
    "    plt.ylabel(\"SVD3\")\n",
    "    plt.ylim([-2.2, 3])\n",
    "    plt.show()\n",
    "\n",
    "    # 3D\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(x, y, z, c=clusters, marker='o', alpha=0.5)\n",
    "\n",
    "    ax.set_title(f\"{title} clustering\")\n",
    "    ax.set_xlabel(\"SVD1\")\n",
    "    ax.set_ylabel(\"SVD2\")\n",
    "    ax.set_zlabel(\"SVD3\")\n",
    "    ax.set_ylim(-2.5, 8)\n",
    "    ax.set_zlim(-2.5, 2.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ingredient_clusters(ingredients_svd, ingredients_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "What does the graph look like ?<br>\n",
    "Are the clusters well-separated ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "I originally tried with 12 clusters and they don't really look well separated. 8 looks marginally better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Silhouette Plots\n",
    "\n",
    "We can also check the quality of our clustering by looking at the silhouette plots that we get from calling:<br>\n",
    "`display_silhouette_plots(vectors, clusters)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_silhouette_plots(ingredients_vectorized, ingredients_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "How many clusters did you end up with ?<br>\n",
    "How do they look ?<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "I'm going with 8 clusters. Aside from 2 clusters the silhouettes are roughly similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe Completion\n",
    "\n",
    "Ok. On to the main event.\n",
    "\n",
    "Let's create some recipes.\n",
    "\n",
    "We'll do this using a technique similar to what is used for movie/product recommendations. Given an initial set of ingredients, we'll look at recipes that have similar ingredients and \"recommend\" additional ingredients.\n",
    "\n",
    "We already have all of the recipes in our dataset encoded as `tf-idf` vectors. The rest of our algorithm will be something like:\n",
    "1. Start with an initial set of ingredients\n",
    "2. Encode ingredients\n",
    "3. Find a set of recipes that are similar to our list of ingredients\n",
    "4. Find common ingredients that are in the similar recipes, but not in our list of ingredients\n",
    "5. Pick representative ingredient to add to recipe\n",
    "6. Repeat\n",
    "\n",
    "Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initial list of ingredients\n",
    "\n",
    "This is just a string with ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_seed_str = \"chicken onion\" # feel free to change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Encode ingredients\n",
    "\n",
    "Transform the string into a `tf-idf` vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: transform string into sparse vector\n",
    "recipe_seed_vct = tfid.transform([recipe_seed_str])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find similar recipes\n",
    "\n",
    "The meat of the algorithm. No pun intended.\n",
    "\n",
    "In order to find similar recipes, we'll first calculate the distance between our current list of ingredients and all recipes in our dataset.\n",
    "\n",
    "We can start with euclidean distance and later try other kinds, but the overall processing will be the same:\n",
    "\n",
    "1. Start with an empty list to store distances\n",
    "2. Loop over the `tf-idf` recipe vectors and for each vector:\n",
    "   1. Subtract the ingredient list\n",
    "   2. Square the difference (to square a sparse matrix `A`, use `A.multiply(A)`)\n",
    "   3. Sum the terms of the result\n",
    "   4. Take the square root of the sum\n",
    "   5. Append to distance list\n",
    "3. Find the indices of the smallest distances (this operation is called `argsort` and will give us the indices of the recipes that are most similar to our list of ingredients)\n",
    "4. Check the recipes to see if they are indeed similar (`inverse_transform()` the vectors at the indices calculated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argsort a list (get sequence of indices that would sort the list)\n",
    "# https://stackoverflow.com/a/3382369\n",
    "def argsort(L, reverse=False):\n",
    "  return sorted(range(len(L)), key=L.__getitem__, reverse=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: list to keep distances\n",
    "recipe_dists = []\n",
    "\n",
    "# print(ingredients_vectorized.shape[0])\n",
    "# TODO: loop over vectors and append euclidean distances to list\n",
    "for i in range(ingredients_vectorized.shape[0]):\n",
    "    ingredient = ingredients_vectorized[i]\n",
    "#    1. Subtract the ingredient list\n",
    "    diff = recipe_seed_vct - ingredient\n",
    "#    2. Square the difference (to square a sparse matrix `A`, use `A.multiply(A)`)\n",
    "    diff_sq = diff.multiply(diff)\n",
    "#    3. Sum the terms of the result\n",
    "    sum = diff_sq.sum()\n",
    "#    4. Take the square root of the sum\n",
    "    sum_sr = np.sqrt(sum)\n",
    "#    5. Append to distance list\n",
    "    recipe_dists.append(sum_sr)\n",
    "\n",
    "# TODO: argsort list of distances to find indices of similar recipes\n",
    "dist_list = argsort(recipe_dists)\n",
    "\n",
    "# TODO: check first 4 recipes\n",
    "print(dist_list[0])\n",
    "print(dist_list[1])\n",
    "print(dist_list[2])\n",
    "print(dist_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in dist_list[:4]:\n",
    "    print(recipes[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find ingredients to recommend\n",
    "\n",
    "We have a way to get a set of similar recipes with similar ingredients, and now want to find a *meaningful*, or *representative*, ingredient to add to our ingredients list.\n",
    "\n",
    "Let's consider ingredients in the $16$ most similar recipes. What we are trying to do is find an ingredient that is in a lot of these recipes, but not yet in our list of ingredients.\n",
    "\n",
    "There are many possible ways of doing this. We could count the number of times different ingredients show up in these $16$ recipes using Python dictionaries and/or sets, but what we're trying to do here is very similar to what a `TfidfVectorizer` does: calculate relative importance of terms in a series of documents.\n",
    "\n",
    "Let's re-encode these $16$ recipes using their own separate `TfidfVectorizer`, then sum the importance of each ingredient and look at ingredients with the highest importance scores.\n",
    "\n",
    "We could re-use the vectors/scores from the original `TfidfVectorizer`, but they're gonna be influenced by the relative frequencies of all of the ingredients that showed up in all of the recipes. Using a separate vectorizer is a little bit more precise.\n",
    "\n",
    "The steps we need to take are:\n",
    "\n",
    "1. Separate the $16$ recipes most similar to our list of ingredients\n",
    "   1. We have lots of representations of our recipes, but `recipes` (list of strings) might be the easiest one to use here\n",
    "2. Create a new `TfidfVectorizer` and encode the $16$ recipes\n",
    "3. Sum the resulting vectors to get overall importance scores for each ingredient/token\n",
    "4. Convert resulting vector to a list using `A.tolist()[0]`\n",
    "5. `argsort` the importance scores to get sequence of ingredient indices ordered from most to least important\n",
    "6. Find the most important ingredient that isn't on the ingredient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get 16 most similar recipes\n",
    "most_similar = []\n",
    "for idx in dist_list[:16]:\n",
    "    most_similar.append(recipes[idx])\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode the 16 recipes\n",
    "tfid_similar = TfidfVectorizer(stop_words=\"english\", max_features=10_000)\n",
    "similar_vectorized = tfid_similar.fit_transform(most_similar)\n",
    "# print(similar_vectorized)\n",
    "\n",
    "vocab_similar = tfid_similar.get_feature_names_out()\n",
    "print(len(vocab_similar))\n",
    "display(vocab_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sum the recipe vectors by column to get ingredient importance scores\n",
    "# similar_vectorized.shape\n",
    "importance = similar_vectorized.sum(axis=0)\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert sparse vector to regular list with A.tolist()[0]\n",
    "importance_list = importance.tolist()[0]\n",
    "print(len(importance_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: argsort the importance scores\n",
    "dist_importance_list = argsort(importance_list)\n",
    "dist_importance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Find most important ingredient not yet on the list of ingredients\n",
    "vocab_similar[dist_importance_list[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Add ingredient to recipe\n",
    "\n",
    "This is simply adding a word to `recipe_seed_str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: add the first important ingredient to list of ingredients\n",
    "recipe_seed_str += \" \" + vocab_similar[dist_importance_list[0]]\n",
    "print(recipe_seed_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Repeat (Optional)\n",
    "\n",
    "Now we can repeat this process until we get an empty list of important ingredients: \n",
    "1. Encode current recipe\n",
    "2. Find similar recipes\n",
    "3. Find important ingredients\n",
    "4. Add important ingredient\n",
    "\n",
    "Might be helpful to define a couple of functions, like `find_similar_recipes()` and `find_important_ingredients()`...\n",
    "\n",
    "Only do this step if you're really curious about experimenting with generating unconventional ingredient lists. It's not going to be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Create find_similar_recipes(ingredients, recipes, vectorizer)\n",
    "\n",
    "# TODO: Create find_important_ingredients(recipes)\n",
    "\n",
    "# TODO: Create recipe by repeating calls to find_similar_recipes() and find_important_ingredients()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
